<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aniki&#39;s Blog</title>
    <link>https://whitewolf2000ani.github.io/AnikiBlogs/</link>
    <description>Recent content on Aniki&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://whitewolf2000ani.github.io/AnikiBlogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A VsCode extension for Mini-WDL</title>
      <link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/building-the-mini-wdl-vs-code-extension/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/building-the-mini-wdl-vs-code-extension/</guid>
      <description>Introduction Creating a VS Code extension for Mini-WDL was an exciting journey filled with learning, debugging, and problem-solving. This blog details how I implemented syntax highlighting and language configuration for Mini-WDL, along with the challenges I faced‚Äîparticularly around marking comments correctly‚Äîand how I overcame them. Let‚Äôs dive in!
 1. Setting Up the Project **Step 1: Followed the documentation VsCode To start, I used Yeoman‚Äôs generator to create the basic structure of the VS Code extension:</description>
    </item>
    
    <item>
      <title>ASTx to Python Mapping Strategy</title>
      <link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/astx-python-ast-mapping/</link>
      <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/astx-python-ast-mapping/</guid>
      <description>1. Expression Node Mapping ASTx BinaryOp ‚Üí Python ast.BinOp ASTx Structure:
BinaryOp( lhs: Expr, rhs: Expr, op: BinaryOpKind ) Python AST Equivalent:
ast.BinOp( left: ast.expr, op: operator, right: ast.expr ) Conversion Logic:
# Operator mapping table OP_MAP = { BinaryOpKind.add: ast.Add(), BinaryOpKind.sub: ast.Sub(), BinaryOpKind.mul: ast.Mult() } @dispatch def visit(self, node: BinaryOp) -&amp;gt; ast.BinOp: return ast.BinOp( left=self.visit(node.lhs), op=OP_MAP[node.op], right=self.visit(node.rhs) )  2. Statement Node Mapping ASTx DeleteStmt ‚Üí Python ast.Delete ASTx Structure:</description>
    </item>
    
    <item>
      <title>Implementing Open LLM Models with JAX and Flax</title>
      <link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/implementing-open-llm-models-with-jax-and-flax/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/implementing-open-llm-models-with-jax-and-flax/</guid>
      <description>Before diving into the implementation details, I want to summarize our approach: we&amp;rsquo;ll be creating JAX/Flax implementations of popular open-source LLM architectures, documenting everything thoroughly, and providing clear notebooks to demonstrate their usage.
Project Overview JAX, combined with Flax, provides a powerful framework for implementing high-performance neural networks with benefits like JIT compilation, automatic differentiation, and excellent hardware acceleration support. Our goal is to create clean, well-documented implementations of open-source LLM architectures that can serve as reference material and starting points for further research.</description>
    </item>
    
    <item>
      <title>Fine-Tuning Models</title>
      <link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/fine-tuning-models-in-google-colab_-a-practical-gu/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/fine-tuning-models-in-google-colab_-a-practical-gu/</guid>
      <description>Google Collab provides free GPU/TPU resources perfect for fine-tuning AI models. Here&amp;rsquo;s a complete guide to fine-tuning models in Colab, from setup to saving your trained model.
Setting Up Your Colab Environment # 1. Connect to a GPU runtime # Go to Runtime &amp;gt; Change runtime type &amp;gt; GPU # 2. Verify GPU is available !nvidia-smi # 3. Install necessary libraries !pip install -q transformers datasets accelerate peft bitsandbytes trl tensorboard Method 1: QLoRA Fine-Tuning for Large Models This approach is ideal for 7B+ parameter models on Colab&amp;rsquo;s limited GPU:</description>
    </item>
    
    <item>
      <title>Obsidian Notes -&gt; Blogs</title>
      <link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/my-blogging-setup/</link>
      <pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/my-blogging-setup/</guid>
      <description>Why am I doing this?  A Repository or a store for whatever cool concepts I study/Understand. It&amp;rsquo;s really important to learn in Public.  How am I doing this?  Obsidian https://obsidian.md/  Hugo for building the static website blazingly fast.  Why are we using HUGO? The simple answer is HUGO converts Markdown files to Website code directly(Isn&amp;rsquo;t this convenientüòÅ)
 Prerequisites for Hugo  Git -&amp;gt; Install According to Operating System.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://whitewolf2000ani.github.io/AnikiBlogs/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://whitewolf2000ani.github.io/AnikiBlogs/about/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
