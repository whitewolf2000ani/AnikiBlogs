<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Aniki's Blog</title><link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/</link><description>Recent content in Posts on Aniki's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 21 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://whitewolf2000ani.github.io/AnikiBlogs/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Implementing Open LLM Models with JAX and Flax</title><link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/implementing-open-llm-models-with-jax-and-flax/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/implementing-open-llm-models-with-jax-and-flax/</guid><description>Before diving into the implementation details, I want to summarize our approach: we&amp;rsquo;ll be creating JAX/Flax implementations of popular open-source LLM architectures, documenting everything thoroughly, and providing clear notebooks to demonstrate their usage.
Project Overview JAX, combined with Flax, provides a powerful framework for implementing high-performance neural networks with benefits like JIT compilation, automatic differentiation, and excellent hardware acceleration support. Our goal is to create clean, well-documented implementations of open-source LLM architectures that can serve as reference material and starting points for further research.</description><content>&lt;hr>
&lt;p>Before diving into the implementation details,
I want to summarize our approach:
we&amp;rsquo;ll be creating JAX/Flax implementations of popular open-source LLM architectures, documenting everything thoroughly, and providing clear notebooks to demonstrate their usage.&lt;/p>
&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>JAX, combined with Flax, provides a powerful framework for implementing high-performance neural networks with benefits like JIT compilation, automatic differentiation, and excellent hardware acceleration support. Our goal is to create clean, well-documented implementations of open-source LLM architectures that can serve as reference material and starting points for further research.&lt;/p>
&lt;h2 id="implementation-roadmap">Implementation Roadmap&lt;/h2>
&lt;h3 id="phase-1-environment-setup-and-core-components">Phase 1: Environment Setup and Core Components&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Development Environment Setup&lt;/strong>
&lt;ul>
&lt;li>Install JAX with hardware-specific optimizations (GPU/TPU)&lt;/li>
&lt;li>Install Flax, Optax (optimizers), and supporting libraries&lt;/li>
&lt;li>Configure development environment with appropriate compute resources&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Implement Core Architecture Components&lt;/strong>
&lt;ul>
&lt;li>Token embedding layers&lt;/li>
&lt;li>Various positional encoding mechanisms (sinusoidal, learned, rotary)&lt;/li>
&lt;li>Attention mechanisms (multi-head attention with causal masking)&lt;/li>
&lt;li>Feed-forward networks&lt;/li>
&lt;li>Normalization layers (LayerNorm, RMSNorm)&lt;/li>
&lt;li>Complete transformer blocks&lt;/li>
&lt;li>Model definition classes with initialization and forward functions&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="phase-2-model-implementations">Phase 2: Model Implementations&lt;/h3>
&lt;p>We&amp;rsquo;ll implement several open-source LLM architectures, starting with simpler models and progressing to more complex ones:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>GPT-2 Style Model&lt;/strong>
&lt;ul>
&lt;li>Decoder-only transformer architecture&lt;/li>
&lt;li>LayerNorm and learned positional embeddings&lt;/li>
&lt;li>Support for various model sizes (124M to 1.5B parameters)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Gemma Architecture&lt;/strong>
&lt;ul>
&lt;li>Google&amp;rsquo;s efficient model developed specifically with JAX/Flax&lt;/li>
&lt;li>RMSNorm and rotary positional embeddings&lt;/li>
&lt;li>2B and 7B parameter configurations&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Additional Models (Time Permitting)&lt;/strong>
&lt;ul>
&lt;li>OpenLLaMA (open-source implementation of LLaMA)&lt;/li>
&lt;li>Mistral (with mixture-of-experts layers)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>For each model, we&amp;rsquo;ll implement:&lt;/p>
&lt;ul>
&lt;li>Complete model definition classes&lt;/li>
&lt;li>Initialization from scratch and from pre-trained weights&lt;/li>
&lt;li>Forward pass functions optimized with JAX transformations&lt;/li>
&lt;li>Text generation utilities&lt;/li>
&lt;/ul>
&lt;h3 id="phase-3-utility-functions-and-optimization">Phase 3: Utility Functions and Optimization&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Weight Loading Utilities&lt;/strong>
&lt;ul>
&lt;li>Parameter key remapping between different naming schemes&lt;/li>
&lt;li>Shape and data type conversion utilities&lt;/li>
&lt;li>Loading from HuggingFace model repositories&lt;/li>
&lt;li>Checkpoint saving/loading with Orbax&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Inference and Generation&lt;/strong>
&lt;ul>
&lt;li>Greedy decoding implementation&lt;/li>
&lt;li>Sampling-based generation with temperature control&lt;/li>
&lt;li>Top-k and top-p (nucleus) sampling&lt;/li>
&lt;li>Batched inference support&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Performance Optimization&lt;/strong>
&lt;ul>
&lt;li>JIT compilation for faster inference&lt;/li>
&lt;li>Vectorization with vmap for batched processing&lt;/li>
&lt;li>Device parallelism with pmap for multi-GPU/TPU setups&lt;/li>
&lt;li>Memory optimization techniques like gradient checkpointing&lt;/li>
&lt;li>Mixed precision support (bfloat16/fp16)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="phase-4-validation-and-documentation">Phase 4: Validation and Documentation&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Validation Against Reference Implementations&lt;/strong>
&lt;ul>
&lt;li>Compare outputs with HuggingFace reference models&lt;/li>
&lt;li>Validate hidden states and logits using similarity metrics&lt;/li>
&lt;li>Verify tokenizer consistency&lt;/li>
&lt;li>Test text generation capabilities&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Documentation and Notebooks&lt;/strong>
&lt;ul>
&lt;li>Comprehensive model documentation&lt;/li>
&lt;li>Jupyter notebooks demonstrating usage&lt;/li>
&lt;li>Performance benchmarks&lt;/li>
&lt;li>Best practices for working with JAX/Flax models&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="technical-challenges-and-solutions">Technical Challenges and Solutions&lt;/h2>
&lt;h3 id="api-compatibility">API Compatibility&lt;/h3>
&lt;p>Flax is transitioning from the Linen API to the newer NNX API. We&amp;rsquo;ll need to handle compatibility by:&lt;/p>
&lt;ol>
&lt;li>Using the flax.nnx.bridge API to convert between Linen and NNX modules&lt;/li>
&lt;li>Properly handling RNG keys and variable collections&lt;/li>
&lt;li>Testing thoroughly to ensure compatibility with different versions&lt;/li>
&lt;/ol>
&lt;h3 id="memory-management-for-large-models">Memory Management for Large Models&lt;/h3>
&lt;p>For larger models, we&amp;rsquo;ll implement:&lt;/p>
&lt;ol>
&lt;li>Gradient checkpointing to reduce memory usage during training&lt;/li>
&lt;li>Model parallelism strategies using JAX&amp;rsquo;s device mesh and partition specs&lt;/li>
&lt;li>Efficient parameter handling to minimize memory overhead&lt;/li>
&lt;/ol>
&lt;h3 id="performance-optimization">Performance Optimization&lt;/h3>
&lt;p>To achieve optimal performance, we&amp;rsquo;ll:&lt;/p>
&lt;ol>
&lt;li>Use JAX&amp;rsquo;s transformation functions (jit, vmap, pmap) appropriately&lt;/li>
&lt;li>Apply XLA optimizations through JAX&lt;/li>
&lt;li>Implement custom kernels where necessary using jax.lax operations&lt;/li>
&lt;li>Leverage scan for sequential operations&lt;/li>
&lt;/ol>
&lt;h2 id="repository-structure">Repository Structure&lt;/h2>
&lt;pre tabindex="0">&lt;code>jax-flax-llms/
‚îú‚îÄ‚îÄ models/
‚îÇ ‚îú‚îÄ‚îÄ components.py (shared transformer components)
‚îÇ ‚îú‚îÄ‚îÄ gpt2/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ model.py (model definition)
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.py (model configuration)
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ utils.py (model-specific utilities)
‚îÇ ‚îú‚îÄ‚îÄ gemma/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ model.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ utils.py
‚îÇ ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ utils/
‚îÇ ‚îú‚îÄ‚îÄ loading.py (weight loading utilities)
‚îÇ ‚îú‚îÄ‚îÄ generation.py (text generation functions)
‚îÇ ‚îú‚îÄ‚îÄ optimization.py (performance optimization)
‚îÇ ‚îî‚îÄ‚îÄ validation.py (validation against references)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ ‚îú‚îÄ‚îÄ 01_gpt2_tutorial.ipynb
‚îÇ ‚îú‚îÄ‚îÄ 02_gemma_tutorial.ipynb
‚îÇ ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ tests/
‚îÇ ‚îú‚îÄ‚îÄ test_components.py
‚îÇ ‚îú‚îÄ‚îÄ test_gpt2.py
‚îÇ ‚îú‚îÄ‚îÄ test_gemma.py
‚îÇ ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
&lt;/code>&lt;/pre>&lt;h2 id="implementation-approach-for-each-model">Implementation Approach for Each Model&lt;/h2>
&lt;p>For each model (using GPT-2 as an example):&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Architecture Research&lt;/strong>
&lt;ul>
&lt;li>Study the original architecture in detail&lt;/li>
&lt;li>Identify key components and parameter configurations&lt;/li>
&lt;li>Understand tokenization and preprocessing requirements&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Core Implementation&lt;/strong>
&lt;ul>
&lt;li>Define the model class structure&lt;/li>
&lt;li>Implement all necessary layers and components&lt;/li>
&lt;li>Create forward pass function with JAX optimizations&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Weight Loading&lt;/strong>
&lt;ul>
&lt;li>Create mapping between original weights and our implementation&lt;/li>
&lt;li>Implement conversion functions for loading pre-trained weights&lt;/li>
&lt;li>Test with published checkpoints&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Inference and Generation&lt;/strong>
&lt;ul>
&lt;li>Implement text generation capabilities&lt;/li>
&lt;li>Optimize for inference speed using JAX transformations&lt;/li>
&lt;li>Support various decoding strategies&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Documentation and Examples&lt;/strong>
&lt;ul>
&lt;li>Create comprehensive model documentation&lt;/li>
&lt;li>Develop clear notebooks showing initialization, loading, and generation&lt;/li>
&lt;li>Include performance benchmarks&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="tools-and-dependencies">Tools and Dependencies&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Core Libraries&lt;/strong>
&lt;ul>
&lt;li>JAX and JAXLIB (with GPU/TPU support)&lt;/li>
&lt;li>Flax (neural network library)&lt;/li>
&lt;li>Optax (optimizers)&lt;/li>
&lt;li>Orbax (checkpointing)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Support Libraries&lt;/strong>
&lt;ul>
&lt;li>Transformers (for reference models and tokenizers)&lt;/li>
&lt;li>NumPy and SciPy (numerical computing)&lt;/li>
&lt;li>Matplotlib (visualization)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Development Tools&lt;/strong>
&lt;ul>
&lt;li>Jupyter notebooks (for examples and demonstrations)&lt;/li>
&lt;li>PyTest (for testing)&lt;/li>
&lt;li>GitHub (for version control and publication)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="educational-focus">Educational Focus&lt;/h2>
&lt;p>Since this project is primarily educational, we&amp;rsquo;ll emphasize:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Clear, Well-Documented Code&lt;/strong>
&lt;ul>
&lt;li>Comprehensive docstrings&lt;/li>
&lt;li>Explanatory comments for complex sections&lt;/li>
&lt;li>Consistent style and naming conventions&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conceptual Understanding&lt;/strong>
&lt;ul>
&lt;li>Explain architecture decisions in documentation&lt;/li>
&lt;li>Compare implementation choices with original models&lt;/li>
&lt;li>Highlight JAX/Flax-specific optimizations&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Practical Examples&lt;/strong>
&lt;ul>
&lt;li>Step-by-step notebooks for different use cases&lt;/li>
&lt;li>Performance comparison between optimization strategies&lt;/li>
&lt;li>Tips and best practices for working with JAX/Flax&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>This project will create a valuable educational resource for researchers and developers interested in implementing LLMs with JAX and Flax. By providing clear, optimized implementations of popular open-source architectures, along with comprehensive documentation and examples, we&amp;rsquo;ll help bridge the gap between theoretical understanding and practical implementation.&lt;/p>
&lt;p>The end result will be a GitHub repository showcasing these implementations, ready for others to use as reference material or starting points for their own research and experimentation.&lt;/p>
&lt;!-- raw HTML omitted --></content></item><item><title>Fine-Tuning Models</title><link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/fine-tuning-models-in-google-colab_-a-practical-gu/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/fine-tuning-models-in-google-colab_-a-practical-gu/</guid><description>Google Collab provides free GPU/TPU resources perfect for fine-tuning AI models. Here&amp;rsquo;s a complete guide to fine-tuning models in Colab, from setup to saving your trained model.
Setting Up Your Colab Environment # 1. Connect to a GPU runtime # Go to Runtime &amp;gt; Change runtime type &amp;gt; GPU # 2. Verify GPU is available !nvidia-smi # 3. Install necessary libraries !pip install -q transformers datasets accelerate peft bitsandbytes trl tensorboard Method 1: QLoRA Fine-Tuning for Large Models This approach is ideal for 7B+ parameter models on Colab&amp;rsquo;s limited GPU:</description><content>&lt;p>Google Collab provides free GPU/TPU resources perfect for fine-tuning AI models. Here&amp;rsquo;s a complete guide to fine-tuning models in Colab, from setup to saving your trained model.&lt;/p>
&lt;h2 id="setting-up-your-colab-environment">Setting Up Your Colab Environment&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#75715e"># 1. Connect to a GPU runtime&lt;/span>
&lt;span style="color:#75715e"># Go to Runtime &amp;gt; Change runtime type &amp;gt; GPU&lt;/span>
&lt;span style="color:#75715e"># 2. Verify GPU is available&lt;/span>
&lt;span style="color:#960050;background-color:#1e0010">!&lt;/span>nvidia&lt;span style="color:#f92672">-&lt;/span>smi
&lt;span style="color:#75715e"># 3. Install necessary libraries&lt;/span>
&lt;span style="color:#960050;background-color:#1e0010">!&lt;/span>pip install &lt;span style="color:#f92672">-&lt;/span>q transformers datasets accelerate peft bitsandbytes trl tensorboard
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="method-1-qlora-fine-tuning-for-large-models">Method 1: QLoRA Fine-Tuning for Large Models&lt;/h2>
&lt;p>This approach is ideal for 7B+ parameter models on Colab&amp;rsquo;s limited GPU:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#75715e"># Import libraries&lt;/span>
&lt;span style="color:#f92672">import&lt;/span> torch
&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
&lt;span style="color:#f92672">from&lt;/span> peft &lt;span style="color:#f92672">import&lt;/span> LoraConfig, get_peft_model
&lt;span style="color:#f92672">from&lt;/span> datasets &lt;span style="color:#f92672">import&lt;/span> load_dataset
&lt;span style="color:#f92672">from&lt;/span> trl &lt;span style="color:#f92672">import&lt;/span> SFTTrainer, SFTConfig
&lt;span style="color:#75715e"># Configure quantization&lt;/span>
bnb_config &lt;span style="color:#f92672">=&lt;/span> BitsAndBytesConfig(
load_in_4bit&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,
bnb_4bit_quant_type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;nf4&amp;#34;&lt;/span>,
bnb_4bit_compute_dtype&lt;span style="color:#f92672">=&lt;/span>torch&lt;span style="color:#f92672">.&lt;/span>float16
)
&lt;span style="color:#75715e"># Load model and tokenizer&lt;/span>
model_id &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;meta-llama/Llama-2-7b-hf&amp;#34;&lt;/span> &lt;span style="color:#75715e"># Or any compatible model&lt;/span>
tokenizer &lt;span style="color:#f92672">=&lt;/span> AutoTokenizer&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_id)
tokenizer&lt;span style="color:#f92672">.&lt;/span>pad_token &lt;span style="color:#f92672">=&lt;/span> tokenizer&lt;span style="color:#f92672">.&lt;/span>eos_token
model &lt;span style="color:#f92672">=&lt;/span> AutoModelForCausalLM&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
model_id,
quantization_config&lt;span style="color:#f92672">=&lt;/span>bnb_config,
device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>
)
&lt;span style="color:#75715e"># Configure LoRA&lt;/span>
lora_config &lt;span style="color:#f92672">=&lt;/span> LoraConfig(
r&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">16&lt;/span>,
lora_alpha&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">32&lt;/span>,
lora_dropout&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.05&lt;/span>,
bias&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;none&amp;#34;&lt;/span>,
task_type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;CAUSAL_LM&amp;#34;&lt;/span>,
target_modules&lt;span style="color:#f92672">=&lt;/span>[&lt;span style="color:#e6db74">&amp;#34;q_proj&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;v_proj&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;k_proj&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;o_proj&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;gate_proj&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;up_proj&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;down_proj&amp;#34;&lt;/span>]
)
&lt;span style="color:#75715e"># Apply LoRA to model&lt;/span>
model &lt;span style="color:#f92672">=&lt;/span> get_peft_model(model, lora_config)
model&lt;span style="color:#f92672">.&lt;/span>print_trainable_parameters()
&lt;span style="color:#75715e"># Prepare dataset&lt;/span>
&lt;span style="color:#75715e"># Option 1: Load from Hugging Face&lt;/span>
dataset &lt;span style="color:#f92672">=&lt;/span> load_dataset(&lt;span style="color:#e6db74">&amp;#34;Abirate/english_quotes&amp;#34;&lt;/span>, split&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;train&amp;#34;&lt;/span>)
&lt;span style="color:#75715e"># Option 2: Or upload CSV to Colab&lt;/span>
&lt;span style="color:#f92672">from&lt;/span> google.colab &lt;span style="color:#f92672">import&lt;/span> files
uploaded &lt;span style="color:#f92672">=&lt;/span> files&lt;span style="color:#f92672">.&lt;/span>upload() &lt;span style="color:#75715e"># Upload your CSV&lt;/span>
&lt;span style="color:#f92672">import&lt;/span> pandas &lt;span style="color:#66d9ef">as&lt;/span> pd
&lt;span style="color:#f92672">from&lt;/span> datasets &lt;span style="color:#f92672">import&lt;/span> Dataset
df &lt;span style="color:#f92672">=&lt;/span> pd&lt;span style="color:#f92672">.&lt;/span>read_csv(&lt;span style="color:#e6db74">&amp;#34;your_file.csv&amp;#34;&lt;/span>)
dataset &lt;span style="color:#f92672">=&lt;/span> Dataset&lt;span style="color:#f92672">.&lt;/span>from_pandas(df)
&lt;span style="color:#75715e"># Configure trainer&lt;/span>
training_args &lt;span style="color:#f92672">=&lt;/span> SFTConfig(
output_dir&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;./results&amp;#34;&lt;/span>,
num_train_epochs&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>,
per_device_train_batch_size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>,
gradient_accumulation_steps&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>,
learning_rate&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2e-4&lt;/span>,
logging_steps&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">50&lt;/span>,
save_strategy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;epoch&amp;#34;&lt;/span>,
fp16&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,
)
&lt;span style="color:#75715e"># Initialize trainer&lt;/span>
trainer &lt;span style="color:#f92672">=&lt;/span> SFTTrainer(
model&lt;span style="color:#f92672">=&lt;/span>model,
args&lt;span style="color:#f92672">=&lt;/span>training_args,
train_dataset&lt;span style="color:#f92672">=&lt;/span>dataset,
tokenizer&lt;span style="color:#f92672">=&lt;/span>tokenizer,
dataset_text_field&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;quote&amp;#34;&lt;/span> &lt;span style="color:#75715e"># Change to your text column name&lt;/span>
)
&lt;span style="color:#75715e"># Start training&lt;/span>
trainer&lt;span style="color:#f92672">.&lt;/span>train()
&lt;span style="color:#75715e"># Save the model&lt;/span>
peft_model_id &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;my-fine-tuned-model&amp;#34;&lt;/span>
trainer&lt;span style="color:#f92672">.&lt;/span>model&lt;span style="color:#f92672">.&lt;/span>save_pretrained(peft_model_id)
tokenizer&lt;span style="color:#f92672">.&lt;/span>save_pretrained(peft_model_id)
&lt;span style="color:#75715e"># Save to Google Drive (to avoid losing work)&lt;/span>
&lt;span style="color:#f92672">from&lt;/span> google.colab &lt;span style="color:#f92672">import&lt;/span> drive
drive&lt;span style="color:#f92672">.&lt;/span>mount(&lt;span style="color:#e6db74">&amp;#39;/content/drive&amp;#39;&lt;/span>)
&lt;span style="color:#960050;background-color:#1e0010">!&lt;/span>cp &lt;span style="color:#f92672">-&lt;/span>r {peft_model_id} &lt;span style="color:#f92672">/&lt;/span>content&lt;span style="color:#f92672">/&lt;/span>drive&lt;span style="color:#f92672">/&lt;/span>MyDrive&lt;span style="color:#f92672">/&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="method-2-full-fine-tuning-for-smaller-models">Method 2: Full Fine-Tuning for Smaller Models&lt;/h2>
&lt;p>For smaller models (under 3B parameters) that fit fully in Colab&amp;rsquo;s GPU:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#f92672">import&lt;/span> torch
&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
&lt;span style="color:#f92672">from&lt;/span> datasets &lt;span style="color:#f92672">import&lt;/span> load_dataset
&lt;span style="color:#75715e"># Load model and tokenizer&lt;/span>
model_id &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;gpt2&amp;#34;&lt;/span> &lt;span style="color:#75715e"># Or another smaller model&lt;/span>
tokenizer &lt;span style="color:#f92672">=&lt;/span> AutoTokenizer&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_id)
tokenizer&lt;span style="color:#f92672">.&lt;/span>pad_token &lt;span style="color:#f92672">=&lt;/span> tokenizer&lt;span style="color:#f92672">.&lt;/span>eos_token
model &lt;span style="color:#f92672">=&lt;/span> AutoModelForCausalLM&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_id)
&lt;span style="color:#75715e"># Prepare your dataset&lt;/span>
dataset &lt;span style="color:#f92672">=&lt;/span> load_dataset(&lt;span style="color:#e6db74">&amp;#34;imdb&amp;#34;&lt;/span>, split&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;train&amp;#34;&lt;/span>)
&lt;span style="color:#75715e"># Define data preprocessing&lt;/span>
&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">tokenize_function&lt;/span>(examples):
&lt;span style="color:#66d9ef">return&lt;/span> tokenizer(examples[&lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>], padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;max_length&amp;#34;&lt;/span>, truncation&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, max_length&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">512&lt;/span>)
tokenized_datasets &lt;span style="color:#f92672">=&lt;/span> dataset&lt;span style="color:#f92672">.&lt;/span>map(tokenize_function, batched&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;span style="color:#75715e"># Configure training&lt;/span>
training_args &lt;span style="color:#f92672">=&lt;/span> TrainingArguments(
output_dir&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;./results&amp;#34;&lt;/span>,
num_train_epochs&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>,
per_device_train_batch_size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8&lt;/span>,
save_strategy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;epoch&amp;#34;&lt;/span>,
evaluation_strategy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;epoch&amp;#34;&lt;/span>,
logging_dir&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;./logs&amp;#34;&lt;/span>,
)
&lt;span style="color:#75715e"># Initialize trainer&lt;/span>
trainer &lt;span style="color:#f92672">=&lt;/span> Trainer(
model&lt;span style="color:#f92672">=&lt;/span>model,
args&lt;span style="color:#f92672">=&lt;/span>training_args,
train_dataset&lt;span style="color:#f92672">=&lt;/span>tokenized_datasets,
)
&lt;span style="color:#75715e"># Start training&lt;/span>
trainer&lt;span style="color:#f92672">.&lt;/span>train()
&lt;span style="color:#75715e"># Save the model&lt;/span>
model&lt;span style="color:#f92672">.&lt;/span>save_pretrained(&lt;span style="color:#e6db74">&amp;#34;./fine-tuned-gpt2&amp;#34;&lt;/span>)
tokenizer&lt;span style="color:#f92672">.&lt;/span>save_pretrained(&lt;span style="color:#e6db74">&amp;#34;./fine-tuned-gpt2&amp;#34;&lt;/span>)
&lt;span style="color:#75715e"># Save to Google Drive&lt;/span>
&lt;span style="color:#960050;background-color:#1e0010">!&lt;/span>cp &lt;span style="color:#f92672">-&lt;/span>r &lt;span style="color:#f92672">./&lt;/span>fine&lt;span style="color:#f92672">-&lt;/span>tuned&lt;span style="color:#f92672">-&lt;/span>gpt2 &lt;span style="color:#f92672">/&lt;/span>content&lt;span style="color:#f92672">/&lt;/span>drive&lt;span style="color:#f92672">/&lt;/span>MyDrive&lt;span style="color:#f92672">/&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="dealing-with-colab-limitations">Dealing with Colab Limitations&lt;/h2>
&lt;h3 id="session-timeouts">Session Timeouts&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#75715e"># Add this to prevent idle timeouts&lt;/span>
&lt;span style="color:#f92672">from&lt;/span> IPython.display &lt;span style="color:#f92672">import&lt;/span> display, Javascript
display(Javascript(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;span style="color:#e6db74">function ClickConnect(){
&lt;/span>&lt;span style="color:#e6db74"> console.log(&amp;#34;Clicking connect button&amp;#34;);
&lt;/span>&lt;span style="color:#e6db74"> document.querySelector(&amp;#34;colab-connect-button&amp;#34;).click()
&lt;/span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;span style="color:#e6db74">setInterval(ClickConnect, 60000)
&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;&lt;/span>))
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="memory-management">Memory Management&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#75715e"># Clear GPU memory if needed&lt;/span>
&lt;span style="color:#f92672">import&lt;/span> gc
gc&lt;span style="color:#f92672">.&lt;/span>collect()
torch&lt;span style="color:#f92672">.&lt;/span>cuda&lt;span style="color:#f92672">.&lt;/span>empty_cache()
&lt;span style="color:#75715e"># Monitor memory usage&lt;/span>
&lt;span style="color:#960050;background-color:#1e0010">!&lt;/span>nvidia&lt;span style="color:#f92672">-&lt;/span>smi
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="checkpointing">Checkpointing&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#75715e"># Configure checkpointing for recovery&lt;/span>
training_args &lt;span style="color:#f92672">=&lt;/span> TrainingArguments(
&lt;span style="color:#75715e"># ... other args&lt;/span>
save_strategy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;steps&amp;#34;&lt;/span>,
save_steps&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">500&lt;/span>,
save_total_limit&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#75715e"># Keep only the last 2 checkpoints&lt;/span>
)
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="testing-your-fine-tuned-model">Testing Your Fine-Tuned Model&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#75715e"># Load your fine-tuned model&lt;/span>
&lt;span style="color:#f92672">from&lt;/span> peft &lt;span style="color:#f92672">import&lt;/span> PeftModel, PeftConfig
&lt;span style="color:#75715e"># For LoRA models&lt;/span>
config &lt;span style="color:#f92672">=&lt;/span> PeftConfig&lt;span style="color:#f92672">.&lt;/span>from_pretrained(&lt;span style="color:#e6db74">&amp;#34;my-fine-tuned-model&amp;#34;&lt;/span>)
model &lt;span style="color:#f92672">=&lt;/span> AutoModelForCausalLM&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
config&lt;span style="color:#f92672">.&lt;/span>base_model_name_or_path,
torch_dtype&lt;span style="color:#f92672">=&lt;/span>torch&lt;span style="color:#f92672">.&lt;/span>float16,
device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>
)
model &lt;span style="color:#f92672">=&lt;/span> PeftModel&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model, &lt;span style="color:#e6db74">&amp;#34;my-fine-tuned-model&amp;#34;&lt;/span>)
&lt;span style="color:#75715e"># Generate text&lt;/span>
tokenizer &lt;span style="color:#f92672">=&lt;/span> AutoTokenizer&lt;span style="color:#f92672">.&lt;/span>from_pretrained(&lt;span style="color:#e6db74">&amp;#34;my-fine-tuned-model&amp;#34;&lt;/span>)
inputs &lt;span style="color:#f92672">=&lt;/span> tokenizer(&lt;span style="color:#e6db74">&amp;#34;Your prompt text here&amp;#34;&lt;/span>, return_tensors&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;pt&amp;#34;&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>to(&lt;span style="color:#e6db74">&amp;#34;cuda&amp;#34;&lt;/span>)
outputs &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(&lt;span style="color:#f92672">**&lt;/span>inputs, max_length&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">100&lt;/span>)
print(tokenizer&lt;span style="color:#f92672">.&lt;/span>decode(outputs[&lt;span style="color:#ae81ff">0&lt;/span>], skip_special_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>))
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="best-practices-for-colab-fine-tuning">Best Practices for Colab Fine-Tuning&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Always connect to Google Drive&lt;/strong> to prevent losing work if Collab disconnects&lt;/li>
&lt;li>&lt;strong>Start small&lt;/strong> - Test your pipeline with a tiny subset of data before full training&lt;/li>
&lt;li>&lt;strong>Monitor GPU usage&lt;/strong> regularly with &lt;code>!nvidia-smi&lt;/code>&lt;/li>
&lt;li>&lt;strong>Use QLoRA for large models&lt;/strong> - It&amp;rsquo;s the most efficient way to fine-tune on Colab&lt;/li>
&lt;li>&lt;strong>Save checkpoints frequently&lt;/strong> and to Google Drive&lt;/li>
&lt;li>&lt;strong>Close other tabs&lt;/strong> in your browser to prevent Collab from disconnecting due to inactivity&lt;/li>
&lt;/ol>
&lt;p>This guide provides everything you need to fine-tune models in Google Collab, from small BERT variants all the way to 7B+ parameter models using memory-efficient techniques.&lt;/p></content></item><item><title>Obsidian Notes -> Blogs</title><link>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/my-blogging-setup/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://whitewolf2000ani.github.io/AnikiBlogs/posts/blogs/my-blogging-setup/</guid><description>Why am I doing this? A Repository or a store for whatever cool concepts I study/Understand. It&amp;rsquo;s really important to learn in Public. How am I doing this? Obsidian https://obsidian.md/ Hugo for building the static website blazingly fast. Why are we using HUGO? The simple answer is HUGO converts Markdown files to Website code directly(Isn&amp;rsquo;t this convenientüòÅ)
Prerequisites for Hugo Git -&amp;gt; Install According to Operating System.</description><content>&lt;h2 id="why-am-i-doing-this">Why am I doing this?&lt;/h2>
&lt;ul>
&lt;li>A Repository or a store for whatever cool concepts I study/Understand.&lt;/li>
&lt;li>It&amp;rsquo;s really important to learn in Public.&lt;/li>
&lt;/ul>
&lt;h2 id="how-am-i-doing-this">How am I doing this?&lt;/h2>
&lt;ul>
&lt;li>Obsidian &lt;a href="https://obsidian.md/">https://obsidian.md/&lt;/a>&lt;/li>
&lt;li>&lt;img src="https://whitewolf2000ani.github.io/AnikiBlogs/images/Pasted_image_20241203121609.png" alt="Image Description">&lt;/li>
&lt;li>Hugo for building the static website blazingly fast.&lt;/li>
&lt;/ul>
&lt;h2 id="why-are-we-using-hugo">Why are we using HUGO?&lt;/h2>
&lt;p>The simple answer is HUGO converts Markdown files to Website code directly(Isn&amp;rsquo;t this convenientüòÅ)&lt;/p>
&lt;ul>
&lt;li>Prerequisites for Hugo
&lt;ul>
&lt;li>Git -&amp;gt; Install According to Operating System.
&lt;ul>
&lt;li>Prompt: How do Install Git in &amp;ldquo;Operating System&amp;rdquo;.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Go -&amp;gt; Install According to Operating System.
&lt;ul>
&lt;li>Prompt: How to Install Go in &amp;ldquo;Operating system&amp;rdquo;.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Install HUGO following the official documentation &lt;a href="https://gohugo.io/installation/linux/">https://gohugo.io/installation/linux/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="the-next-steps">The next Steps&lt;/h2>
&lt;ul>
&lt;li>Create a Folder for your blogs.&lt;/li>
&lt;li>Move into that folder and use command&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">hugo new site &amp;lt;NameOfSite&amp;gt;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>The required files for HUGO will be created.&lt;/li>
&lt;li>Initialize a Git repository&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">git init
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Choose a exciting theme of choice. -&amp;gt; &lt;a href="https://themes.gohugo.io/">https://themes.gohugo.io/&lt;/a>
&lt;ul>
&lt;li>After choosing a theme, -&amp;gt; &lt;a href="https://themes.gohugo.io/themes/hugo-theme-terminal/">https://themes.gohugo.io/themes/hugo-theme-terminal/&lt;/a>&lt;/li>
&lt;li>Find &amp;ldquo;Install theme as submodule&amp;rdquo; this is the easiest and best way to use the Theme.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">//similar to this command
git submodule add -f https://github.com/panr/hugo-theme-terminal.git themes/terminal
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>We need to configure the &amp;ldquo;config.toml&amp;rdquo; file in order to render the theme (e.g. Config file for terminal theme)&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-config.toml" data-lang="config.toml">
&lt;span style="color:#960050;background-color:#1e0010">//&lt;/span> &lt;span style="color:#a6e22e">Example&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span> &lt;span style="color:#a6e22e">for&lt;/span> &lt;span style="color:#a6e22e">terminal&lt;/span> &lt;span style="color:#a6e22e">theme&lt;/span>
&lt;span style="color:#a6e22e">baseurl&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;/&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">languageCode&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;en-us&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># Add it only if you keep the theme in the `themes` directory. &lt;/span>
&lt;span style="color:#75715e"># Remove it if you use the theme as a remote Hugo Module. &lt;/span>
&lt;span style="color:#a6e22e">theme&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;terminal&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">paginate&lt;/span> = &lt;span style="color:#ae81ff">5&lt;/span>
[&lt;span style="color:#a6e22e">params&lt;/span>]
&lt;span style="color:#75715e"># dir name of your main content (default is `content/posts`). &lt;/span>
&lt;span style="color:#75715e"># the list of set content will show up on your index page (baseurl). &lt;/span>
&lt;span style="color:#a6e22e">contentTypeName&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;posts&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># if you set this to 0, only submenu trigger will be visible &lt;/span>
&lt;span style="color:#a6e22e">showMenuItems&lt;/span> = &lt;span style="color:#ae81ff">2&lt;/span>
&lt;span style="color:#75715e"># show selector to switch language &lt;/span>
&lt;span style="color:#a6e22e">showLanguageSelector&lt;/span> = &lt;span style="color:#66d9ef">false&lt;/span>
&lt;span style="color:#75715e"># set theme to full screen width &lt;/span>
&lt;span style="color:#a6e22e">fullWidthTheme&lt;/span> = &lt;span style="color:#66d9ef">false&lt;/span>
&lt;span style="color:#75715e"># center theme with default width &lt;/span>
&lt;span style="color:#a6e22e">centerTheme&lt;/span> = &lt;span style="color:#66d9ef">false&lt;/span>
&lt;span style="color:#75715e"># if your resource directory contains an image called `cover.(jpg|png|webp)`, &lt;/span>
&lt;span style="color:#75715e"># then the file will be used as a cover automatically. &lt;/span>
&lt;span style="color:#75715e"># With this option you don&amp;#39;t have to put the `cover` param in a front-matter. &lt;/span>
&lt;span style="color:#a6e22e">autoCover&lt;/span> = &lt;span style="color:#66d9ef">true&lt;/span>
&lt;span style="color:#75715e"># set post to show the last updated # If you use git, you can set `enableGitInfo` to `true` and then post will automatically get the last updated &lt;/span>
&lt;span style="color:#a6e22e">showLastUpdated&lt;/span> = &lt;span style="color:#66d9ef">false&lt;/span>
&lt;span style="color:#75715e"># Provide a string as a prefix for the last update date. By default, it looks like this: 2020-xx-xx [Updated: 2020-xx-xx] :: Author &lt;/span>
&lt;span style="color:#75715e"># updatedDatePrefix = &amp;#34;Updated&amp;#34; # whether to show a page&amp;#39;s estimated reading time # readingTime = false # default &lt;/span>
&lt;span style="color:#75715e"># whether to show a table of contents &lt;/span>
&lt;span style="color:#75715e"># can be overridden in a page&amp;#39;s front-matter &lt;/span>
&lt;span style="color:#75715e"># Toc = false # default &lt;/span>
&lt;span style="color:#75715e"># set title for the table of contents &lt;/span>
&lt;span style="color:#75715e"># can be overridden in a page&amp;#39;s front-matter &lt;/span>
&lt;span style="color:#75715e"># TocTitle = &amp;#34;Table of Contents&amp;#34; &lt;/span>
&lt;span style="color:#75715e"># default [params.twitter] &lt;/span>
&lt;span style="color:#75715e"># set Twitter handles for Twitter cards &lt;/span>
&lt;span style="color:#75715e"># see https://developer.twitter.com/en/docs/tweets/optimize-with-cards/guides/getting-started#card-and-content-attribution &lt;/span>
&lt;span style="color:#75715e"># do not include &lt;/span>
&lt;span style="color:#960050;background-color:#1e0010">@&lt;/span> &lt;span style="color:#a6e22e">creator&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">site&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
[&lt;span style="color:#a6e22e">languages&lt;/span>]
[&lt;span style="color:#a6e22e">languages&lt;/span>.&lt;span style="color:#a6e22e">en&lt;/span>]
&lt;span style="color:#a6e22e">languageName&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;English&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">title&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Terminal&amp;#34;&lt;/span>
[&lt;span style="color:#a6e22e">languages&lt;/span>.&lt;span style="color:#a6e22e">en&lt;/span>.&lt;span style="color:#a6e22e">params&lt;/span>]
&lt;span style="color:#a6e22e">subtitle&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;A simple, retro theme for Hugo&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">owner&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">keywords&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">copyright&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">menuMore&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Show more&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">readMore&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Read more&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">readOtherPosts&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Read other posts&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">newerPosts&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Newer posts&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">olderPosts&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Older posts&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">missingContentMessage&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Page not found...&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">missingBackButtonLabel&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Back to home page&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">minuteReadingTime&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;min read&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">words&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;words&amp;#34;&lt;/span>
[&lt;span style="color:#a6e22e">languages&lt;/span>.&lt;span style="color:#a6e22e">en&lt;/span>.&lt;span style="color:#a6e22e">params&lt;/span>.&lt;span style="color:#a6e22e">logo&lt;/span>]
&lt;span style="color:#a6e22e">logoText&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Terminal&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">logoHomeLink&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;/&amp;#34;&lt;/span>
[&lt;span style="color:#a6e22e">languages&lt;/span>.&lt;span style="color:#a6e22e">en&lt;/span>.&lt;span style="color:#a6e22e">menu&lt;/span>]
[[&lt;span style="color:#a6e22e">languages&lt;/span>.&lt;span style="color:#a6e22e">en&lt;/span>.&lt;span style="color:#a6e22e">menu&lt;/span>.&lt;span style="color:#a6e22e">main&lt;/span>]]
&lt;span style="color:#a6e22e">identifier&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;about&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">name&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;About&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">url&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;/about&amp;#34;&lt;/span>
[[&lt;span style="color:#a6e22e">languages&lt;/span>.&lt;span style="color:#a6e22e">en&lt;/span>.&lt;span style="color:#a6e22e">menu&lt;/span>.&lt;span style="color:#a6e22e">main&lt;/span>]]
&lt;span style="color:#a6e22e">identifier&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;showcase&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">name&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;Showcase&amp;#34;&lt;/span>
&lt;span style="color:#a6e22e">url&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;/showcase&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>After this a simple command of&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">hugo server -t &amp;lt;themename&amp;gt;
&lt;/code>&lt;/pre>&lt;h2 id="now-lets-see-some-text-rendering-">Now lets see some text rendering? üñãÔ∏è&lt;/h2>
&lt;ul>
&lt;li>But there seems to be problemüòê, the obsidian blog folder and our hugo blog folder are in different locations.&lt;/li>
&lt;li>The content in them should sync simultaneously.&lt;/li>
&lt;li>We will solve this problem using a very specific command both in linux and Windows.&lt;/li>
&lt;/ul>
&lt;p>linux&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">rsync -av --delete &amp;quot;sourcepath&amp;quot; &amp;quot;destinationpath&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Windows&lt;/p>
&lt;pre tabindex="0">&lt;code>robocopy sourcepath destination path /mir
&lt;/code>&lt;/pre>&lt;h2 id="now-lets-render-the-images">Now lets render the images&lt;/h2>
&lt;ul>
&lt;li>One more problem that we see here is the rendering of images.&lt;/li>
&lt;li>The problem arises because obsidian keeps a different attachments folder for media.&lt;/li>
&lt;li>Now we need to copy the images used in our blog to the Hugo codebase.&lt;/li>
&lt;li>So we will use a python script to copy all the used images from the attachments folder to folder inside static/images/&lt;/li>
&lt;li>Use this python script naming it as images.py and run it to see the magicüòÅ.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">&lt;span style="color:#f92672">import&lt;/span> os
&lt;span style="color:#f92672">import&lt;/span> re
&lt;span style="color:#f92672">import&lt;/span> shutil
&lt;span style="color:#75715e"># Paths&lt;/span>
posts_dir &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;lt;Blog.md Destination inside&amp;gt;&amp;#34;&lt;/span>
attachments_dir &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">r&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;lt;Path where obsidian stores its images&amp;gt;&amp;#34;&lt;/span>
static_images_dir &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;lt;Path of static/images directory where you want to copy your images&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># Base URL for Hugo&lt;/span>
baseURL &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;https://whitewolf2000ani.github.io/AnikiBlogs&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># Ensure static images directory exists&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>exists(static_images_dir):
¬† ¬† os&lt;span style="color:#f92672">.&lt;/span>makedirs(static_images_dir)
&lt;span style="color:#75715e"># Function to normalize image names (replace spaces with underscores)&lt;/span>
&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">normalize_image_name&lt;/span>(image_name):
¬† ¬† &lt;span style="color:#66d9ef">return&lt;/span> image_name&lt;span style="color:#f92672">.&lt;/span>replace(&lt;span style="color:#e6db74">&amp;#34; &amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;_&amp;#34;&lt;/span>)
¬† ¬†
&lt;span style="color:#75715e"># Process each Markdown file in the posts directory&lt;/span>
&lt;span style="color:#66d9ef">for&lt;/span> filename &lt;span style="color:#f92672">in&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>listdir(posts_dir):
¬† ¬† &lt;span style="color:#66d9ef">if&lt;/span> filename&lt;span style="color:#f92672">.&lt;/span>endswith(&lt;span style="color:#e6db74">&amp;#34;.md&amp;#34;&lt;/span>):
¬† ¬† ¬† ¬† filepath &lt;span style="color:#f92672">=&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>join(posts_dir, filename)
¬† ¬† ¬† ¬† &lt;span style="color:#66d9ef">with&lt;/span> open(filepath, &lt;span style="color:#e6db74">&amp;#34;r&amp;#34;&lt;/span>) &lt;span style="color:#66d9ef">as&lt;/span> file:
¬† ¬† ¬† ¬† ¬† ¬† content &lt;span style="color:#f92672">=&lt;/span> file&lt;span style="color:#f92672">.&lt;/span>read()
¬† ¬† ¬† ¬† ¬† ¬†
¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Find all image links in the format `![[filename.extension]]`&lt;/span>
¬† ¬† ¬† ¬† images &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>findall(&lt;span style="color:#e6db74">r&lt;/span>&lt;span style="color:#e6db74">&amp;#34;!\[\[([^]]+\.(?:png|jpg|jpeg|gif|webp))\]\]&amp;#34;&lt;/span>, content)
¬† ¬† ¬† ¬† &lt;span style="color:#66d9ef">for&lt;/span> image &lt;span style="color:#f92672">in&lt;/span> images:
¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Normalize the image filename&lt;/span>
¬† ¬† ¬† ¬† ¬† ¬† normalized_image_name &lt;span style="color:#f92672">=&lt;/span> normalize_image_name(image)
¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Original and normalized filenames&lt;/span>
¬† ¬† ¬† ¬† ¬† ¬† image_with_spaces &lt;span style="color:#f92672">=&lt;/span> image
¬† ¬† ¬† ¬† ¬† ¬† image_with_encoded_spaces &lt;span style="color:#f92672">=&lt;/span> normalized_image_name
¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Path to the original image&lt;/span>
¬† ¬† ¬† ¬† ¬† ¬† image_source &lt;span style="color:#f92672">=&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>join(attachments_dir, image_with_spaces)
¬† ¬† ¬† ¬† ¬† ¬† print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Checking for image: &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>image_source&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Replace Markdown link with a Hugo-compatible link&lt;/span>
¬† ¬† ¬† ¬† ¬† ¬† new_image_link &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;![Image Description](&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>baseURL&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">/images/&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>image_with_encoded_spaces&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">)&amp;#34;&lt;/span>
¬† ¬† ¬† ¬† ¬† ¬† content &lt;span style="color:#f92672">=&lt;/span> content&lt;span style="color:#f92672">.&lt;/span>replace(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;![[&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>image_with_spaces&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">]]&amp;#34;&lt;/span>, new_image_link)
¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Copy the image to the static directory if it exists&lt;/span>
¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#66d9ef">if&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>exists(image_source):
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Copy the image to the static directory with a normalized name&lt;/span>
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† shutil&lt;span style="color:#f92672">.&lt;/span>copy(image_source, os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>join(static_images_dir, normalized_image_name))
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Copied: &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>image_source&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74"> -&amp;gt; &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>static_images_dir&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">/&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>normalized_image_name&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
¬† ¬† ¬† ¬† ¬† ¬† &lt;span style="color:#66d9ef">else&lt;/span>:
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Image not found: &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>image_source&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†
¬† ¬† ¬† ¬† &lt;span style="color:#75715e"># Write updated content back to the file&lt;/span>
¬† ¬† ¬† ¬† &lt;span style="color:#66d9ef">with&lt;/span> open(filepath, &lt;span style="color:#e6db74">&amp;#34;w&amp;#34;&lt;/span>) &lt;span style="color:#66d9ef">as&lt;/span> file:
¬† ¬† ¬† ¬† ¬† ¬† file&lt;span style="color:#f92672">.&lt;/span>write(content)
print(&lt;span style="color:#e6db74">&amp;#34;Markdown files processed and images copied successfully.&amp;#34;&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>After using the below script run.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-terminal" data-lang="terminal">python3 images.py
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>This will copy all the images used in the current blog to the images folder inside static.
&lt;img src="https://whitewolf2000ani.github.io/AnikiBlogs/images/Pasted_image_20241203173907.png" alt="Image Description">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After the images are copied.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run the file syncing command.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Start the Hugo server.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Whoosh the image has been rendered on our localhost.
&lt;img src="https://whitewolf2000ani.github.io/AnikiBlogs/images/Pasted_image_20241203181859.png" alt="Image Description">&lt;/p>
&lt;h2 id="do-we-always-have-to-run-the-sync-command-and-then-copy-the-images">Do we always have to run the sync command and then copy the images?&lt;/h2>
&lt;ul>
&lt;li>The simple answer is yes, but we will automate the entire task using a python script at the end of the blog.&lt;/li>
&lt;/ul>
&lt;h2 id="now-lets-put-this-out-in-the-openpun-intended">Now let&amp;rsquo;s put this out in the open(Pun intended)üòÅ&lt;/h2>
&lt;ul>
&lt;li>Remember the &amp;ldquo;git init&amp;rdquo; command we used to initialize a git repository, now we will use the local repo.&lt;/li>
&lt;li>Step 1: Go to &lt;a href="http://www.github.com">www.github.com&lt;/a>&lt;/li>
&lt;li>Step 2: Create a new repository.&lt;/li>
&lt;li>step 3: Connect the local repository to your remote repository.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">git remote add origin &amp;lt;/ssh /https link&amp;gt;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>step 4: Run Hugo to create all the necessary changes.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">hugo
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Step 5: Now add all the changes made.&lt;/li>
&lt;li>Step 6: Commit these changes with a valid comment.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">git add.
git commit -m &amp;quot;My, first commit to the blog&amp;quot;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Step 7: Push these changes to GitHub/ remote.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>git push -u origin master
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Now we will see the changes in our Remote GitHub Repository(or simply on the GitHub Website).&lt;/li>
&lt;/ul>
&lt;h2 id="now-the-moment-of-truth-where-should-we-host-our-website-">Now the moment of Truth, where should we host our website ü§î?&lt;/h2>
&lt;ul>
&lt;li>Vercel/ GitHub pages/ Hostinger.&lt;/li>
&lt;li>I will be using GitHub pages.
&lt;ul>
&lt;li>It&amp;rsquo;s good for static websites.&lt;/li>
&lt;li>It&amp;rsquo;s free üòç.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Let&amp;rsquo;s do it.&lt;/li>
&lt;li>Step 1: We are creating a separate branch for deploying the Blog.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">git checkout --orphan gh-pages
git rm -rf .
echo &amp;quot;This is the gh-pages branch&amp;quot; &amp;gt; README.md
git add README.md
git commit -m &amp;quot;Initialize gh-pages branch&amp;quot;
git push origin gh-pages
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Step 2: Configure GitHub pages.
&lt;ul>
&lt;li>Inside your blogs repository there is click on the settings icon.&lt;/li>
&lt;li>Navigate to the Code and Automation Section.&lt;/li>
&lt;li>Click on Actions.&lt;/li>
&lt;li>Click on General.
&lt;img src="https://whitewolf2000ani.github.io/AnikiBlogs/images/Pasted_image_20241204215856.png" alt="Image Description">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Under the Workflow permissions-&amp;gt; click on Read and Write permissions.&lt;/li>
&lt;li>Step 3: Under the Pages section set the source to gh-pages branch and /root directory.&lt;/li>
&lt;li>Step 4: Now checkout to the master branch&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">git checkout master
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Step 5: Create the &lt;code>.github/workflows/deploy.yml&lt;/code> file with the deployment instructions.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-deploy.yml" data-lang="deploy.yml">&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">Deploy Hugo to GitHub Pages&lt;/span>
&lt;span style="color:#f92672">on&lt;/span>:
&lt;span style="color:#f92672">push&lt;/span>:
&lt;span style="color:#f92672">branches&lt;/span>:
- &lt;span style="color:#ae81ff">master &lt;/span> &lt;span style="color:#75715e"># Your main branch name&lt;/span>
&lt;span style="color:#f92672">jobs&lt;/span>:
&lt;span style="color:#f92672">deploy&lt;/span>:
&lt;span style="color:#f92672">runs-on&lt;/span>: &lt;span style="color:#ae81ff">ubuntu-latest&lt;/span>
&lt;span style="color:#f92672">steps&lt;/span>:
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">Checkout code&lt;/span>
&lt;span style="color:#f92672">uses&lt;/span>: &lt;span style="color:#ae81ff">actions/checkout@v3&lt;/span>
&lt;span style="color:#f92672">with&lt;/span>:
&lt;span style="color:#f92672">submodules&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span> &lt;span style="color:#75715e"># Fetch submodules if any&lt;/span>
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">Install Hugo&lt;/span>
&lt;span style="color:#f92672">run&lt;/span>: |&lt;span style="color:#e6db74">
&lt;/span>&lt;span style="color:#e6db74"> wget https://github.com/gohugoio/hugo/releases/download/v0.92.2/hugo_extended_0.92.2_Linux-64bit.tar.gz
&lt;/span>&lt;span style="color:#e6db74"> tar -xzf hugo_extended_0.92.2_Linux-64bit.tar.gz -C /usr/local/bin&lt;/span>
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">Build site&lt;/span>
&lt;span style="color:#f92672">run&lt;/span>: &lt;span style="color:#ae81ff">hugo --minify&lt;/span>
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">Deploy to gh-pages&lt;/span>
&lt;span style="color:#f92672">uses&lt;/span>: &lt;span style="color:#ae81ff">peaceiris/actions-gh-pages@v3&lt;/span>
&lt;span style="color:#f92672">with&lt;/span>:
&lt;span style="color:#f92672">github_token&lt;/span>: &lt;span style="color:#ae81ff">${{ secrets.GITHUB_TOKEN }}&lt;/span>
&lt;span style="color:#f92672">publish_dir&lt;/span>: &lt;span style="color:#ae81ff">./public&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Use the corrected &lt;code>master&lt;/code> branch for your main branch and &lt;code>gh-pages&lt;/code> for deployment.&lt;/li>
&lt;li>Step 6: Ensure your &lt;code>hugo&lt;/code> configuration points to the correct &lt;code>baseURL&lt;/code> in &lt;code>config.toml&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-config.toml" data-lang="config.toml">&lt;span style="color:#a6e22e">baseURL&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;https://&amp;lt;username&amp;gt;.github.io/&amp;lt;repo-name&amp;gt;/&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Step 7: Run these commands below to start the workflow now.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-Terminal" data-lang="Terminal">hugo
git add .
git commit -m &amp;quot;Add GitHub Pages deployment workflow&amp;quot;
git push origin master
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Step 8: Take a breather till the actions are getting completed.&lt;/li>
&lt;li>Step 9: After the actions are completed go the same settings/ pages option and open the deployment link.(As the link you are using to read the blog)&lt;/li>
&lt;li>Step 10: Here you go your Blog site is up and running for everyone to see.üòç&lt;/li>
&lt;/ul>
&lt;h2 id="lets-summarize">Let&amp;rsquo;s Summarize&lt;/h2>
&lt;ul>
&lt;li>We write our content inside Obsidian.&lt;/li>
&lt;li>Copy this .md file into our Blogs directory using rsync/robocopy&lt;/li>
&lt;li>We create a copy of the images used in our blog to the &lt;code>static/images&lt;/code> folder using &lt;code>images.py&lt;/code>&lt;/li>
&lt;li>Now we push it to our master branch after which the &lt;code>deployment.yml&lt;/code> deploys it to our GitHub page using the &lt;code>gh-pg&lt;/code> branch.&lt;br>
Ughh the process seems so cumbersome ü§î.&lt;/li>
&lt;/ul>
&lt;h2 id="the-ultimate-python-script-for-one-command-notes--blog">The Ultimate python script for one command notes-&amp;gt; blog&lt;/h2>
&lt;ul>
&lt;li>Save the script with &lt;code>.sh&lt;/code> extension&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#75715e">#!/bin/bash
&lt;/span>&lt;span style="color:#75715e">&lt;/span>set -euo pipefail
&lt;span style="color:#75715e"># Change to the script&amp;#39;s directory&lt;/span>
SCRIPT_DIR&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>cd &lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>dirname &lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>BASH_SOURCE[0]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> pwd&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
cd &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$SCRIPT_DIR&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># Set variables for Obsidian to Hugo copy&lt;/span>
sourcePath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;lt;source in obsidian&amp;gt;&amp;#34;&lt;/span>
destinationPath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;lt;Destination inside hugo content/blogs&amp;gt;&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># Set GitHub Repo&lt;/span>
myrepo&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;reponame&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># Check for required commands&lt;/span>
&lt;span style="color:#66d9ef">for&lt;/span> cmd in git rsync python3 hugo; &lt;span style="color:#66d9ef">do&lt;/span>
¬† ¬† &lt;span style="color:#66d9ef">if&lt;/span> ! command -v $cmd &amp;amp;&amp;gt; /dev/null; &lt;span style="color:#66d9ef">then&lt;/span>
¬†¬† ¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$cmd&lt;span style="color:#e6db74"> is not installed or not in PATH.&amp;#34;&lt;/span>
¬† ¬† ¬† ¬† exit &lt;span style="color:#ae81ff">1&lt;/span>
¬† ¬† &lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#66d9ef">done&lt;/span>
&lt;span style="color:#75715e"># Step 1: Check if Git is initialized, and initialize if necessary&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> ! git remote | grep -q &lt;span style="color:#e6db74">&amp;#39;origin&amp;#39;&lt;/span>; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† git remote add origin &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$myrepo&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Remote &amp;#39;origin&amp;#39; added.&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">else&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Remote &amp;#39;origin&amp;#39; already exists.&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#75715e"># Step 2: Sync posts from Obsidian to Hugo content folder using rsync&lt;/span>
echo &lt;span style="color:#e6db74">&amp;#34;Syncing posts from Obsidian...&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">[&lt;/span> ! -d &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$sourcePath&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Source path does not exist: &lt;/span>$sourcePath&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
¬† ¬† exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">[&lt;/span> ! -d &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$destinationPath&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Destination path does not exist: &lt;/span>$destinationPath&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
¬† ¬† exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
rsync -av --delete &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$sourcePath&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$destinationPath&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;span style="color:#75715e"># Step 3: Process Markdown files with Python script to handle image links&lt;/span>
echo &lt;span style="color:#e6db74">&amp;#34;Processing image links in Markdown files...&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">[&lt;/span> ! -f &lt;span style="color:#e6db74">&amp;#34;images.py&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Python script images.py not found.&amp;#34;&lt;/span>
¬† ¬† exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> ! python3 images.py; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Failed to process image links.&amp;#34;&lt;/span>
¬† ¬† exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#75715e"># Step 4: Build the Hugo site&lt;/span>
echo &lt;span style="color:#e6db74">&amp;#34;Building the Hugo site...&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> ! hugo; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Hugo build failed.&amp;#34;&lt;/span>
¬† ¬† exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#75715e"># Step 5: Add changes to Git&lt;/span>
echo &lt;span style="color:#e6db74">&amp;#34;Staging changes for Git...&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> git diff --quiet &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> git diff --cached --quiet; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;No changes to stage.&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">else&lt;/span>
¬† ¬† git add .
&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#75715e"># Step 6: Commit changes with a dynamic message&lt;/span>
commit_message&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;New Blog Post on &lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>date +&lt;span style="color:#e6db74">&amp;#39;%Y-%m-%d %H:%M:%S&amp;#39;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> git diff --cached --quiet; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;No changes to commit.&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">else&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Committing changes...&amp;#34;&lt;/span>
¬† ¬† git commit -S -m &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$commit_message&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;span style="color:#75715e"># Step 7: Push all changes to the main branch&lt;/span>
echo &lt;span style="color:#e6db74">&amp;#34;Deploying to GitHub Master...&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">if&lt;/span> ! git push origin master; &lt;span style="color:#66d9ef">then&lt;/span>
¬† ¬† echo &lt;span style="color:#e6db74">&amp;#34;Failed to push to master branch.&amp;#34;&lt;/span>
¬† ¬† exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">fi&lt;/span>
echo &lt;span style="color:#e6db74">&amp;#34;All done! Site synced, processed, committed, built, and deployed.&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Gives your script execution permission.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">chmmod +x updateblog.sh
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Now lets run the scipt.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">bash updateblog.sh
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="finally">Finally&lt;/h2>
&lt;ul>
&lt;li>Just make changes to your blog, add more blogs&lt;/li>
&lt;li>go to your terminal, run the magic command &lt;code>bash updateblog.sh&lt;/code>&lt;/li>
&lt;li>That directly puts out your content to the GitHub pages.&lt;/li>
&lt;li>Credits: Network Chuck(&lt;a href="https://youtu.be/dnE7c0ELEH8?si=ctH_oufnQeyGmUkn">https://youtu.be/dnE7c0ELEH8?si=ctH_oufnQeyGmUkn&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="be-tuned-for-more-blogs-">Be Tuned for more blogs ü§ó.&lt;/h2></content></item></channel></rss>